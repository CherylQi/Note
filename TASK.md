# 2019.9.11 

## 任务：Linux + 逻辑回归

1. ##### linux系统初步使用，基础要求如下

   1. Ubuntu系统安装，（16.04版本）
   2. Ubuntu系统中，各种必须软件安装，（pycharm等，（要求安装专业版，自己激活））
   3. 了解系统自带python环境，自带python环境中各种包的安装
   4. 安装anconda，并利用conda命令创建各种虚拟python环境
   5. python虚拟环境的激活退出，以及虚拟环境中python包的安装（注意pip，pip3所对应的python环境）
   6. 掌握linux基本命令

2. ##### python基础

   1. pycharm软件的使用，（如选择不同python环境）
   2. python基础
   3. python各种包的基础使用（numpy，pandas）

3. ##### 进阶：

   1. 逻辑回归+tf-idf +jiaba 分词 实现文本分类（自己找数据集）
      1. 要求：了解机器学习基本流程
      2. 逻辑回归、tf-fidf原理
      3. 文本分类评价指标

## Note

### Ubuntu安装

   windows下磁盘分区时压缩后不分配空间即空闲
   安装时分为四区：先分配交换分区与逻辑分区，最后分配主分区。引导分区为/boot.

|  主分区  |   /   |    12G     |
| :------: | :---: | :--------: |
| 逻辑分区 | /boot |    200M    |
| 逻辑分区 | /home |  尽可能大  |
| 逻辑分区 | swap  | 4G物理内存 |

​    回到windows下载easyBCD。 添加Ubuntu选项

### **python基础**

​                            （python2 python3）安装anaconda 后默认python3
  							虚拟环境安python包：进入各环境后pip
 							 conda环境激活：conda activate 退出：conda deactivate
 							 创建环境：virtualenv --no-site-packages venv
​	  					    激活：source venv/bin/activate
  							conda命令创建环境：conda create -n name python=version

### **进阶 逻辑回归+tf-idf +jiaba 分词 实现文本分类**

#### logistic 回归

- 回归：假设现在有一些数据点，我们用一条直线对这些点进行拟合

- 逻辑回归：用于解决二分类(0 or 1)的问题的机器学习方法，用于估计某种事物的可能性

- sigmoid函数：常被用作神经网络的阈值函数，将变量映射到0，1之间，为了实现Logistic回归分类器，可以在每个特征上都乘以一个回归系数，然后把所有结果值相加，将这个总和代入sigmoid函数中，进而得到一个范围在0～1之间的数值

  ```
  算法：每个回归系数初始化为1
  
        重复R次：
  
           计算整个数据集的梯度
  
           使用步长*梯度 更新回归系数的向量
  
        返回回归系数
  ```

  

 **logistic回归的一般过程：**
 **收集数据：**采用任意方法收集数据
 **准备数据：**由于需要进行距离计算，因此要求数据类型为数值型。另外，结构化数据格式最佳
 **分析数据：**采用任意方法对数据进行分析 
 **训练算法：**大部分时间将用于训练，训练的目的是为了找到最佳的分类回归系数
 **测试算法：**一旦训练步骤完成，分类将会很快
 **使用算法：**首先，我们需要输入一些数据，并将其转换成对应的结构化数值；接着，基于训练 好的回归系数就可以对这些数值进行简单的回归计算，判定它们属于哪个类别；在 这之后，我们就可以在输出的类别上做一些其他分析工作。 

       logistic回归
        优点：计算代价不高，易于理解和实现
        缺点：容易欠拟合，分类精度可能不高          适用数值类型：数值型和标称型数据	

#### tfidf

原理：统计方法，用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度，字词的重要性随着它在文件中出现的次数成正比增加，但同时随着它在语料库中出现的频率成反比下降

①词频(TF)：某个词在文章中出现次数/文章总词数

②逆文档频率(IDF)：  
$$
log\frac{语料库的文档总数}{包含该词的文档数+1}
$$
③TF-IDF=TF * IDF

**表示形式：**（句子序号，词典中的位置）  tf-idf值

TfidfVectorizer() 完成向量化与TF-IDF预处理

**token_pattern**
这个参数使用正则表达式来分词，其默认参数为r"(?u)\b\w\w+\b"，其中的两个\w决定了其匹配长度至少为2的单词，所以这边减到1个。

**ngram_range: tuple(min_n, max_n)**
要提取的n-gram的n-values的下限和上限范围，在min_n <= n <= max_n区间的n的全部值

**max_df： float in range [0.0, 1.0] or int, optional, 1.0 by default**
当构建词汇表时，严格忽略高于给出阈值的文档频率的词条，语料指定的停用词。如果是浮点值，该参数代表文档的比例，整型绝对计数值，如果词汇表不为None，此参数被忽略。

**min_df：float in range [0.0, 1.0] or int, optional, 1.0 by default**
当构建词汇表时，严格忽略低于给出阈值的文档频率的词条，语料指定的停用词。如果是浮点值，该参数代表文档的比例，整型绝对计数值，如果词汇表不为None，此参数被忽略。

**fit(): Method calculates the parameters μ and σ and saves them as internal objects.**
解释：简单来说，就是求得训练集X的均值，方差，最大值，最小值,这些训练集X固有的属性。

**transform(): Method using these calculated parameters apply the transformation to a particular dataset.**

解释：在fit的基础上，进行标准化，降维，归一化等操作（看具体用的是哪个工具，如PCA，StandardScaler等）。

**fit_transform(): joins the fit() and transform() method for transformation of dataset.**
解释：fit_transform是fit和transform的组合，既包括了训练又包含了转换。
transform()和fit_transform()二者的功能都是对数据进行某种统一处理（比如标准化~N(0,1)，将数据缩放(映射)到某个固定区间，归一化，正则化等）

**fit_transform(trainData)**

对部分数据先拟合fit，找到该part的整体指标，如均值、方差、最大值最小值等等（根据具体转换的目的），然后对该trainData进行转换transform，从而实现数据的标准化、归一化等等。

(1) 导入模型。调用逻辑回归LogisticRegression()函数。

(2) fit()训练。调用fit(x,y)的方法来训练模型，其中x为数据的属性，y为所属类型。

(3) predict()预测。利用训练得到的模型对数据集进行预测，返回预测结果。

penalty惩罚项  str, ‘l1’ or ‘l2’,  默认: ‘l2’
**注：**在调参时如果我们主要的目的只是为了解决过拟合，一般penalty选择L2正则化就够了。但是如果选择L2正则化发现还是过拟合，即预测效果差的时候，就可以考虑L1正则化。另外，如果模型的特征非常多，我们希望一些不重要的特征系数归零，从而让模型系数稀疏化的话，也可以使用L1正则化。

solver：优化算法选择参数，只有五个可选参数，即newton-cg,lbfgs,liblinear,sag,saga。默认为liblinear。solver参数决定了我们对逻辑回归损失函数的优化方法，有四种算法可以选择，分别是：
    liblinear：使用了开源的liblinear库实现，内部使用了坐标轴下降法来迭代优化损失函数。
    lbfgs：拟牛顿法的一种，利用损失函数二阶导数矩阵即海森矩阵来迭代优化损失函数。
    newton-cg：也是牛顿法家族的一种，利用损失函数二阶导数矩阵即海森矩阵来迭代优化损失函数。
    sag：即随机平均梯度下降，是梯度下降法的变种，和普通梯度下降法的区别是每次迭代仅仅用一部分的样本来计算梯度，适合于样本数据多的时候。
    saga：线性收敛的随机优化算法的的变重。
n_jobs：并行数。int类型，默认为1。1的时候，用CPU的一个内核运行程序，2的时候，用CPU的2个内核运行程序。为-1的时候，用所有CPU的内核运行程序。

sklearn中的classification_report函数用于显示主要分类指标的文本报告．在报告中显示每个类的精确度，召回率，F1值等信息。 
主要参数: 
y_true：1维数组，或标签指示器数组/稀疏矩阵，目标值。 
y_pred：1维数组，或标签指示器数组/稀疏矩阵，分类器返回的估计值。 
labels：array，shape = [n_labels]，报表中包含的标签索引的可选列表。 
target_names：字符串列表，与标签匹配的可选显示名称（相同顺序）。 
sample_weight：类似于shape = [n_samples]的数组，可选项，样本权重。 
digits：int，输出浮点值的位数．

**特征选择主要有两个功能：**
减少特征数量、降维，使模型泛化能力更强，减少过拟合
增强对特征和特征值之间的理解



# 2019.9.19

## 贝叶斯 文本分类

### 朴素贝叶斯原理

基于贝叶斯定理与特征条件独立假设的分类方法，属于统计学方法。
$$
P（类别｜特征）= \frac{P(类别 │ 特征) P(类别)}{P(特征)}
$$
优点：对于输入数据的准备方式较为敏感。
适用数据类型：标称型数据。



# 2019.9.26

## 任务：序列标注原理 + HMM原理

|   序列标注   |            HMM             |
| :----------: | :------------------------: |
|     分词     |          HMM原理           |
| 命名实体识别 |        HMM实现分词         |
|              |    HMM实现命名实体识别     |
|              | 实现两个函数(输出分词+NER) |

数据集已经给出

分词数据集： 分词语料库

NER数据集 ：命名实体识别语料



## Note

### 命名实体识别（NER）

识别文本中具有特别意义的词语。

### HMM原理

隐马尔可夫模型是一个关于时序的概率模型，它描述了一个由隐藏的马尔可夫链生成状态序列，再由状态序列生成观测序列的过程。其中，状态之间的转换以及观测序列和状态序列之间都存在一定的概率关系。

https://www.cnblogs.com/liuwu265/p/4732797.html

param obs:观测序列
:param states:隐状态
:param start_p:初始概率（隐状态）
:param trans_p:转移概率（隐状态）
:param emit_p: 发射概率 （隐状态表现为显状态的概率）



预测(filter)：已知模型参数和某一特定输出序列，求最后时刻各个隐含状态的概率分布，即求  通常使用前向算法解决.
平滑(smoothing)：已知模型参数和某一特定输出序列，求中间时刻各个隐含状态的概率分布，即求 通常使用前向-后向算法解决.
解码(most likely explanation): 已知模型参数，寻找最可能的能产生某一特定输出序列的隐含状态的序列. 即求通常使用Viterbi算法解决.


第四周任务：

深度学习实现文本分类：  

1.了解one-hot，词向量原理 

2.两种词向量模型，**Skip-gram**，**CBOW**  

3.学习gensim护照其他词向量训练框架 

4.词向量结合DNN网络实现文本的分类    

1. 利用tensorflow框架，pytorch也行   
2. 采用随机词向量实现   
3. 采用自训练词向量实现 

5.分类效果评价，与之前机器学习的效果对比  

数据集：  	

1. 为十分类数据集  

Note:
one-hot：假设我们的词库总共有n个词，那我们开一个1*n的高维向量，而每个词都会在某个索引index下取到1，其余位置全部都取值为0。
一种是CBOW模型，通过前后n个词（上下文）来预测当前词的概率，隐藏层对上下文词的词向量求和，所以节点数和词向量维度相同。

一种是Skip-gram模型，通过词w预测上下文范围内每个词的概率，所以Skip-gram 中的每个词向量表征了上下文的分布。
